{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8042988,"sourceType":"datasetVersion","datasetId":4740586}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-07T03:07:00.141468Z","iopub.execute_input":"2025-01-07T03:07:00.142048Z","iopub.status.idle":"2025-01-07T03:07:00.146645Z","shell.execute_reply.started":"2025-01-07T03:07:00.142013Z","shell.execute_reply":"2025-01-07T03:07:00.145735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install duckdb\nimport duckdb\ncon = duckdb.connect()\n\ndf_train = con.query(f\"\"\"(SELECT *\n                        FROM parquet_scan('/kaggle/input/belka-shrunken-train-set/train.parquet')\n                        WHERE binds_BRD4 = 0\n                        ORDER BY random()\n                        LIMIT 12000)\n                        UNION ALL\n                        (SELECT *\n                        FROM parquet_scan('/kaggle/input/belka-shrunken-train-set/train.parquet')\n                        WHERE binds_BRD4 = 1\n                        ORDER BY random()\n                        LIMIT 12000)\"\"\").df()\n\ndf_test = con.query(f\"\"\"(SELECT *\n                        FROM parquet_scan('/kaggle/input/belka-shrunken-train-set/test.parquet')\n                        WHERE is_BRD4 = 0\n                        ORDER BY random()\n                        LIMIT 3000)\n                        UNION ALL\n                        (SELECT *\n                        FROM parquet_scan('/kaggle/input/belka-shrunken-train-set/test.parquet')\n                        WHERE is_BRD4 = 1\n                        ORDER BY random()\n                        LIMIT 3000)\"\"\").df()\n\ncon.close()\n\nprint(\"training Data:\", df_train.shape)\nprint(\"testing Data:\", df_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:00.148281Z","iopub.execute_input":"2025-01-07T03:07:00.148629Z","iopub.status.idle":"2025-01-07T03:07:24.391336Z","shell.execute_reply.started":"2025-01-07T03:07:00.148603Z","shell.execute_reply":"2025-01-07T03:07:24.390186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train)","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:24.393843Z","iopub.execute_input":"2025-01-07T03:07:24.394670Z","iopub.status.idle":"2025-01-07T03:07:24.403129Z","shell.execute_reply.started":"2025-01-07T03:07:24.394639Z","shell.execute_reply":"2025-01-07T03:07:24.402277Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\nCounter(df_train[\"binds_HSA\"])\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:24.404535Z","iopub.execute_input":"2025-01-07T03:07:24.404985Z","iopub.status.idle":"2025-01-07T03:07:24.417478Z","shell.execute_reply.started":"2025-01-07T03:07:24.404940Z","shell.execute_reply":"2025-01-07T03:07:24.416615Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Counter(df_train[\"binds_BRD4\"])","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:24.419984Z","iopub.execute_input":"2025-01-07T03:07:24.420598Z","iopub.status.idle":"2025-01-07T03:07:24.431507Z","shell.execute_reply.started":"2025-01-07T03:07:24.420562Z","shell.execute_reply":"2025-01-07T03:07:24.430624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Counter(df_train[\"binds_sEH\"])","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:24.432616Z","iopub.execute_input":"2025-01-07T03:07:24.432905Z","iopub.status.idle":"2025-01-07T03:07:24.442731Z","shell.execute_reply.started":"2025-01-07T03:07:24.432878Z","shell.execute_reply":"2025-01-07T03:07:24.441921Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rdkit","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:24.443780Z","iopub.execute_input":"2025-01-07T03:07:24.444561Z","iopub.status.idle":"2025-01-07T03:07:33.664858Z","shell.execute_reply.started":"2025-01-07T03:07:24.444523Z","shell.execute_reply":"2025-01-07T03:07:33.663733Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import rdFingerprintGenerator\nfrom rdkit import RDLogger # Suppress RDKit warnings RDLogger.DisableLog('rdApp.*')\nimport warnings\nwarnings.filterwarnings('ignore')\n\ngenerator = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024)\n\necfp_train= []\nfor smiles in df_train[\"molecule_smiles\"]:\n    mol = Chem.MolFromSmiles(smiles)\n    if mol:\n        # Generate ECFP fingerprint for the molecule\n        fp = generator.GetFingerprint(mol)\n        # Convert RDKit's ExplicitBitVect to a numpy array\n        fp_array = np.zeros((1024,), dtype=int)\n        Chem.DataStructs.ConvertToNumpyArray(fp, fp_array)\n        ecfp_train.append(fp_array)\n    else:\n        print(f\"Invalid SMILES: {smiles}\")\n        ecfp_train.append(None)    \n\necfp_test = []\nfor smiles in df_test[\"molecule_smiles\"]:\n    mol = Chem.MolFromSmiles(smiles)\n    if mol:\n        fp = generator.GetFingerprint(mol)\n        fp_array = np.zeros((1204), dtype=int)\n        Chem.DataStructs.ConvertToNumpyArray(fp, fp_array)\n        ecfp_test.append(fp_array)\n    else:\n        print(f\"Invalid SMILES: {smiles}\")\n        ecfp_test.append(None)    \n\n\n\n        \necfp_train = np.array([fp for fp in ecfp_train if fp is not None])\necfp_test = np.array([fp for fp in ecfp_test if fp is not None])","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:33.666665Z","iopub.execute_input":"2025-01-07T03:07:33.667046Z","iopub.status.idle":"2025-01-07T03:07:50.781636Z","shell.execute_reply.started":"2025-01-07T03:07:33.667000Z","shell.execute_reply":"2025-01-07T03:07:50.780920Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ecfp_train)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:50.782831Z","iopub.execute_input":"2025-01-07T03:07:50.783495Z","iopub.status.idle":"2025-01-07T03:07:50.789107Z","shell.execute_reply.started":"2025-01-07T03:07:50.783450Z","shell.execute_reply":"2025-01-07T03:07:50.788208Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nX_train = pd.DataFrame(ecfp_train)\nX_test = pd.DataFrame(ecfp_test)\n\ny_train = df_train['binds_BRD4']\ny_test = df_test['is_BRD4']\n\n#print(\"X_train\", X_train)\n#print(\"X_test\", X_test)\n\nlogreg_model = LogisticRegression()\n\nlogreg_model.fit(X_train, y_train) \n\ny_pred = logreg_model.predict(X_test) \naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy:\", accuracy)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:50.790178Z","iopub.execute_input":"2025-01-07T03:07:50.790452Z","iopub.status.idle":"2025-01-07T03:07:52.832158Z","shell.execute_reply.started":"2025-01-07T03:07:50.790428Z","shell.execute_reply":"2025-01-07T03:07:52.830302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, y_pred, labels = [1,0])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"is_HSA\", \"not_HSA\"])\ndisp.plot(cmap='Oranges');","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:52.842497Z","iopub.execute_input":"2025-01-07T03:07:52.843640Z","iopub.status.idle":"2025-01-07T03:07:53.161904Z","shell.execute_reply.started":"2025-01-07T03:07:52.843575Z","shell.execute_reply":"2025-01-07T03:07:53.161031Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_score, recall_score\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)\nprecision = precision_score(y_test, y_pred)\nprint(precision)","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:53.162863Z","iopub.execute_input":"2025-01-07T03:07:53.163095Z","iopub.status.idle":"2025-01-07T03:07:54.611070Z","shell.execute_reply.started":"2025-01-07T03:07:53.163072Z","shell.execute_reply":"2025-01-07T03:07:54.610161Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport matplotlib.pyplot as plt\n\n\n# Split the dataset into training and testing sets (80% train, 20% test)\n\n# Create a Decision Tree Classifier model\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train the model on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"is_HSA\", \"not_HSA\"])\ndisp.plot(cmap='Blues')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:54.612048Z","iopub.execute_input":"2025-01-07T03:07:54.612350Z","iopub.status.idle":"2025-01-07T03:07:58.427465Z","shell.execute_reply.started":"2025-01-07T03:07:54.612322Z","shell.execute_reply":"2025-01-07T03:07:58.426631Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding\nfrom sklearn.model_selection import train_test_split\n\n\n# Assuming X_train and X_test are currently 2D arrays of shape (samples, sequence_length)\nprint(X_train.shape)  # Check the shape before expanding dimensions\n\n# Correctly reshape input to be 3D: (samples, sequence_length, features)\nX_train_3d = np.expand_dims(X_train, axis=-1) \nX_test = np.expand_dims(X_test, axis=-1)   \n\nprint(X_train_3d.shape)  # Check the shape after expansion to confirm it's (samples, sequence_length, 1)\n\nX_val, X_final_test, y_val, y_final_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\n\n# Define the model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(1024, 1)))  # Sequence length 1024, 1 feature\nmodel.add(MaxPooling1D(pool_size=2))\n\nmodel.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(pool_size=2))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n# Train the model\nmodel.fit(X_train_3d, y_train, epochs=6, batch_size=32, validation_data=(X_val, y_val))\n\n# Evaluate the model\nloss, accuracy, precision, recall = model.evaluate(X_final_test, y_final_test)\n\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")\nprint(f\"Test Precision: {precision}\")\nprint(f\"Test Recall: {recall}\")","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:07:58.428820Z","iopub.execute_input":"2025-01-07T03:07:58.429244Z","iopub.status.idle":"2025-01-07T03:08:33.375580Z","shell.execute_reply.started":"2025-01-07T03:07:58.429169Z","shell.execute_reply":"2025-01-07T03:08:33.374760Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 1: Check the shape of X_train and ensure it has 24000 rows and 1024 columns\n# print(X_train.shape)  # Should be (24000, 1024),1024 is length of ecfp\n\n# # Step 2: Ensure y_train is a pandas Series with 24000 elements\n# print(y_train.shape)  # Should be (24000,)\n\n# y_train_series = pd.Series(y_train, name='label')\n\n# print(y_train_series.head())\n# # Step 3: Concatenate along axis 1 (columns)\n# train_df = pd.concat([X_train, y_train_series], axis=1)\n\n# positive_samples = train_df[train_df['label'] == 1].head(5)  # First 5 positive samples\n# negative_samples = train_df[train_df['label'] == 0].head(5)  # First 5 negative samples\n\n# # Print 5 positive samples\n# print(\"Positive samples:\")\n# print(positive_samples)\n\n# # Print 5 negative samples\n# print(\"\\nNegative samples:\")\n# print(negative_samples)\n# cm = confusion_matrix(y_test, y_pred)\n# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"is_HSA\", \"not_HSA\"])\n# disp.plot(cmap='Blues')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:08:33.376525Z","iopub.execute_input":"2025-01-07T03:08:33.376788Z","iopub.status.idle":"2025-01-07T03:08:33.381152Z","shell.execute_reply.started":"2025-01-07T03:08:33.376763Z","shell.execute_reply":"2025-01-07T03:08:33.380369Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rdkit.Chem import Draw\nimport matplotlib.pyplot as plt\nimage_train =[]\nfor smiles in df_train[\"molecule_smiles\"]:\n    mol = Chem.MolFromSmiles(smiles)\n    if mol:\n        # Generate ECFP fingerprint for the molecule\n        img = Draw.MolToImage(mol, size = (100, 100))\n        image_train.append(img)\n    else:\n        print(f\"Invalid SMILES: {smiles}\")\n        image_train.append(None)    \n        \nimage_test =[]\nfor smiles in df_test[\"molecule_smiles\"]:\n    mol = Chem.MolFromSmiles(smiles)\n    if mol:\n        # Generate ECFP fingerprint for the molecule\n        img = Draw.MolToImage(mol, size = (100, 100))\n        image_test.append(img)\n    else:\n        print(f\"Invalid SMILES: {smiles}\")\n        image_test.append(None)    ","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:08:33.382197Z","iopub.execute_input":"2025-01-07T03:08:33.382551Z","iopub.status.idle":"2025-01-07T03:10:42.547134Z","shell.execute_reply.started":"2025-01-07T03:08:33.382515Z","shell.execute_reply":"2025-01-07T03:10:42.546361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\ndef preprocess_image(image, target_size=(100,100)):\n    if image is not None:\n        image = image.resize(target_size)\n        image = image.convert('L')\n        image_array = np.array(image)\n        image_array = (image_array - np.min(image_array)) / (np.max(image_array) - np.min(image_array)) # Normalize\n        return image_array\n    return None\n    \nimage_train_processed = np.array([preprocess_image(img) for img in image_train])\nimage_test_processed = np.array([preprocess_image(img) for img in image_test])\n\n# Remove None entries if any\n#image_train_processed = image_train_processed[~np.array([img is None for img in image_train])]\n#image_test_processed = image_test_processed[~np.array([img is None for img in image_test])]\n\n# Reshape for CNN input (adding channel dimension)\nimage_train_processed = image_train_processed.reshape(-1, 100, 100, 1)  # Shape: (num_samples, 100, 100, 1)\nimage_test_processed = image_test_processed.reshape(-1, 100, 100, 1)  ","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:10:42.548475Z","iopub.execute_input":"2025-01-07T03:10:42.548749Z","iopub.status.idle":"2025-01-07T03:10:56.282882Z","shell.execute_reply.started":"2025-01-07T03:10:42.548721Z","shell.execute_reply":"2025-01-07T03:10:56.282175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of images to display\nnum_images = 10\n\n# Select indices for positive and negative samples\npositive_indices = np.where(y_train == 1)[0][:5]  # First 5 positive samples\nnegative_indices = np.where(y_train == 0)[0][:5]  # First 5 negative samples\n\n# Combine the indices\nindices = np.concatenate([positive_indices, negative_indices])\n\n# Set up the figure\nfig, axes = plt.subplots(2, num_images, figsize=(15, 6))\nfig.suptitle('5 Positive and 5 Negative Samples Training', fontsize=16)\n\n# Iterate through the selected indices and display images\nfor i, idx in enumerate(indices):\n    ax = axes[i // num_images, i % num_images]  # Row and column index\n    ax.imshow(image_train_processed[idx].squeeze(), cmap='gray')  # Squeeze to remove the channel dimension\n    ax.set_title(f'Label: {y_train[idx]}')\n    ax.axis('off')  # Hide axis\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for suptitle\nplt.show()\n\n# Number of images to display\nnum_images = 10\n\n# Select indices for positive and negative samples\npositive_indices = np.where(y_test == 1)[0][:5]  # First 5 positive samples\nnegative_indices = np.where(y_test == 0)[0][:5]  # First 5 negative samples\n\n# Combine the indices\nindices = np.concatenate([positive_indices, negative_indices])\n\n# Set up the figure\nfig, axes = plt.subplots(2, num_images, figsize=(15, 6))\nfig.suptitle('5 Positive and 5 Negative Samples (Testing)', fontsize=16)\n\n# Iterate through the selected indices and display images\nfor i, idx in enumerate(indices):\n    ax = axes[i // num_images, i % num_images]  # Row and column index\n    ax.imshow(image_test_processed[idx].squeeze(), cmap='gray')  # Squeeze to remove the channel dimension\n    ax.set_title(f'Label: {y_test[idx]}')\n    ax.axis('off')  # Hide axis\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for suptitle\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:10:56.283893Z","iopub.execute_input":"2025-01-07T03:10:56.284168Z","iopub.status.idle":"2025-01-07T03:10:59.389163Z","shell.execute_reply.started":"2025-01-07T03:10:56.284142Z","shell.execute_reply":"2025-01-07T03:10:59.388384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nmodel = models.Sequential()\n\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (100,100, 1)))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D(2, 2))\n\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D(2, 2))\n\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(2, 2))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation = 'relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\n#val_image, test_image = train_test_split(image_test_processed, test_size=0.5, random_state=42)\noptimizer = Adam(learning_rate=0.0001)\ndatagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)#data augmentation by adding variability (e.g. rotations, flips, scaling])\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\nhistory = model.fit(datagen.flow(image_train_processed, y_train, batch_size=32), epochs=9, batch_size=32, validation_data = (image_test_processed, y_test), callbacks=[early_stopping])\nloss, accuracy, precision, recall = model.evaluate(image_test_processed, y_test)\n\ndef plot_acc(history):\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n    \nplot_acc(history)\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")\nprint(f\"Test Precision: {precision}\")\nprint(f\"Test Recall: {recall}\")\n\ny_pred = (model.predict(image_test_processed) > 0.5).astype(\"int32\")\n\ncm = confusion_matrix(y_test, y_pred)\ndisplay = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisplay.plot(cmap=plt.cm.Blues)\nplt.title(\"2DCNN Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:10:59.390599Z","iopub.execute_input":"2025-01-07T03:10:59.390945Z","iopub.status.idle":"2025-01-07T03:13:30.109684Z","shell.execute_reply.started":"2025-01-07T03:10:59.390908Z","shell.execute_reply":"2025-01-07T03:13:30.108801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-geometric\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GraphConv\nfrom torch_geometric.data import Data\nfrom rdkit.Chem import rdmolops\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.nn import global_max_pool\n\ndef smiles_to_graph(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        print(f\"Invalid SMILES: {smiles}\")\n        return None\n    adjacency_matrix = rdmolops.GetAdjacencyMatrix(mol)\n    atom_features = []\n    for atom in mol.GetAtoms():\n        atom_features.append([atom.GetAtomicNum()])\n    edge_index = torch.tensor(adjacency_matrix.nonzero(), dtype=torch.long)\n    x = torch.tensor(atom_features, dtype=torch.float)\n    return Data(x=x, edge_index=edge_index)\n\nclass GNN(nn.Module):\n    def __init__(self):\n        super(GNN, self).__init__()\n        self.conv1 = GraphConv(1, 64)\n        self.conv2 =GraphConv(64, 128)\n        self.conv3 = GraphConv(128, 64)\n        self.fc = torch.nn.Linear(64, 1)\n        self.dropout = torch.nn.Dropout(p=0.3)\n       \n      \n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n                            \n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        \n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        \n\n \n        x = global_max_pool(x, batch)  # Global pooling to get graph-level embedding (reduces dimensions of input by taking the maximum value over all elements)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\n    #dtypes = {'buildingblock1_smiles': np.int16, 'buildingblock2_smiles': np.int16, 'buildingblock3_smiles': np.int16,\n#          'binds_BRD4':np.byte, 'binds_HSA':np.byte, 'binds_sEH':np.byte}\n\n#train = pd.read_csv('/kaggle/input/belka-shrunken-train-set/train.csv', dtype = dtypes)\n#print(len(train))\n\n#abridged_train = train.sample(frac=0.00001, random_state = 42)\n#len(abridged_train)  \n#test = pd.read_csv('/kaggle/input/belka-shrunken-train-set/test.csv', dtype = dtypes)\n#print(len(test))\n\n#abridged_test = test.sample(frac=0.001, random_state = 42)\n#len(abridged_test) ","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:13:30.110793Z","iopub.execute_input":"2025-01-07T03:13:30.111057Z","iopub.status.idle":"2025-01-07T03:13:39.399706Z","shell.execute_reply.started":"2025-01-07T03:13:30.111030Z","shell.execute_reply":"2025-01-07T03:13:39.398790Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ncriterion = torch.nn.BCEWithLogitsLoss()\ngraphs_train = []\nfor smiles in df_train['molecule_smiles']:\n    graph = smiles_to_graph(smiles)\n    if graph is not None:\n        graphs_train.append(graph)\n\ngraphs_test = []\nfor smiles in df_test['molecule_smiles']: \n    graph = smiles_to_graph(smiles)\n    if graph is not None:\n        graphs_test.append(graph)\n\n\nfrom torch_geometric.data import Data\n\nfor i, graph in enumerate(graphs_train):\n    graph.y = torch.tensor([y_train[i]], dtype=torch.float)  # Attach each label to its corresponding graph in training set\n\nfor i, graph in enumerate(graphs_test):\n    graph.y = torch.tensor([y_test[i]], dtype=torch.float)  \n\nval_data, train_data = train_test_split(graphs_train, test_size=0.2, random_state=42, shuffle=True, stratify = y_train)#####\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle = True, drop_last = True)\nval_loader = DataLoader(val_data, batch_size=32)\ntest_loader = DataLoader(graphs_test, batch_size=32)\n\nmodel = GNN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\nprint(f\"Train label distribution: {sum(y_train) / len(y_train)}\")\nprint(f\"Test label distribution: {sum(y_test) / len(y_test)}\")\n\nfor epoch in range(10):\n    model.train()\n    correct, total = 0, 0\n    all_preds, all_labels = [], []\n    \n    for data in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        \n            \n        output = output.view(-1)  # Flatten the output tensor\n        target = data.y.float().view(-1)\n    \n        loss = criterion(output,target) #y_pred, y_test\n        loss.backward()\n        optimizer.step()\n    \n        #calculate correct, total, all_preds, and all_labels separately in training and validation to ensure they are reset properly\n        predicted = (torch.sigmoid(output) > 0.5).float()  \n        correct += (predicted == target).sum().item()\n        total += target.size(0)\n        \n        all_preds.extend(predicted.cpu().numpy().flatten())\n        all_labels.extend(target.cpu().numpy().flatten())\n        \n    accuracy = correct / total\n    precision = precision_score(all_labels, all_preds)\n    recall = recall_score(all_labels, all_preds)\n    print(f'Epoch {epoch+1}, Train Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n    \n    \n    \n    \n    # Validation loop after each epoch\n    model.eval()\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = [] \n    with torch.no_grad():\n        for data in val_loader:\n            output = model(data)\n            output = output.view(-1)#output shape matches the batch size\n            target = data.y.float().view(-1)\n            predicted = (torch.sigmoid(output) > 0.5).float()  # Binary prediction\n            \n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy().flatten())\n            all_labels.extend(target.cpu().numpy().flatten())\n    val_accuracy = correct / total\n    val_precision = precision_score(all_labels, all_preds)\n    val_recall = recall_score(all_labels, all_preds)\n    print(f'Epoch {epoch+1}, Validation Accuracy: {val_accuracy:.4f}, Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}')\n\nmodel.eval()\ntest_correct, test_total = 0,0\nall_test_preds, all_test_labels = [], []\n\nwith torch.no_grad():\n    for data in test_loader:\n        output = model(data)\n        predicted = (torch.sigmoid(output)>0.5).float()\n        all_test_preds.extend(predicted.cpu().numpy().flatten())\n        all_test_labels.extend(data.y.cpu().numpy().flatten())\n        \naccuracy = accuracy_score(all_test_labels, all_test_preds)\nprecision = precision_score(all_test_labels, all_test_preds)\nrecall = recall_score(all_test_labels, all_test_preds)\nprint(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\nmodel.eval()\n\ncm = confusion_matrix(all_labels, all_preds)\ndisplay = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisplay.plot(cmap=plt.cm.Blues)\nplt.title(f\"GNN Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:13:39.401436Z","iopub.execute_input":"2025-01-07T03:13:39.402296Z","iopub.status.idle":"2025-01-07T03:14:36.677157Z","shell.execute_reply.started":"2025-01-07T03:13:39.402250Z","shell.execute_reply":"2025-01-07T03:14:36.676448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nfrom torch_geometric.utils import to_networkx\nimport networkx as nx\n\n# Function to plot the graph\ndef plot_graph(graph, label, index):\n    # Convert the PyG graph to a NetworkX graph for visualization\n    G = to_networkx(graph, to_undirected=True)\n    plt.figure(figsize=(4, 4))\n    nx.draw(G, node_size=50, node_color='blue', with_labels=False)\n    plt.title(f'Graph {index} (Label: {label})')\n    plt.show()\n\n# Separate positive and negative samples from graphs_train and graphs_test\npositive_samples_train = [graph for graph in graphs_train if graph.y.item() == 1][:5]\nnegative_samples_train = [graph for graph in graphs_train if graph.y.item() == 0][:5]\n\npositive_samples_test = [graph for graph in graphs_test if graph.y.item() == 1][:5]\nnegative_samples_test = [graph for graph in graphs_test if graph.y.item() == 0][:5]\n\n\nprint(\"Training Positive samples:\")\nfor i, graph in enumerate(positive_samples_train):\n    plot_graph(graph, label=1, index=i)\n\nprint(\"Training Negative samples:\")\nfor i, graph in enumerate(negative_samples_train):\n    plot_graph(graph, label=0, index=i)\n\n\nprint(\"Testing Positive samples\")\nfor i, graph in enumerate(positive_samples_test):\n    plot_graph(graph, label=1, index=i)\n\nprint(\"Testing Negative sample\")\nfor i, graph in enumerate(negative_samples_test):\n    plot_graph(graph, label=0, index=i)\n'''\n\n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:14:36.678114Z","iopub.execute_input":"2025-01-07T03:14:36.678394Z","iopub.status.idle":"2025-01-07T03:14:36.684680Z","shell.execute_reply.started":"2025-01-07T03:14:36.678368Z","shell.execute_reply":"2025-01-07T03:14:36.683813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rdkit.Chem import rdFingerprintGenerator\n\n# Create a MorganGenerator instance\ngenerator = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024)\n\ndef get_ecfp_vector(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol:\n        # Generate ECFP fingerprint for the molecule using MorganGenerator\n        fp = generator.GetFingerprint(mol)\n        # Convert RDKit's ExplicitBitVect to a numpy array\n        fp_array = np.zeros((1024,), dtype=int)\n        Chem.DataStructs.ConvertToNumpyArray(fp, fp_array)\n        return fp_array\n    else:\n        print(f\"Invalid SMILES: {smiles}\")\n        return None  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T03:14:36.685783Z","iopub.execute_input":"2025-01-07T03:14:36.686103Z","iopub.status.idle":"2025-01-07T03:14:36.696512Z","shell.execute_reply.started":"2025-01-07T03:14:36.686066Z","shell.execute_reply":"2025-01-07T03:14:36.695859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rdkit.Chem import Descriptors\ndef calculate_descriptors(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol:\n        molecular_weight = Descriptors.MolWt(mol)\n        logP = Descriptors.MolLogP(mol)\n        tpsa = Descriptors.TPSA(mol)\n        return molecular_weight, logP, tpsa\n    else:\n        return None, None, None  # Return None if SMILES is invalid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T03:14:36.697546Z","iopub.execute_input":"2025-01-07T03:14:36.698412Z","iopub.status.idle":"2025-01-07T03:14:36.704670Z","shell.execute_reply.started":"2025-01-07T03:14:36.698370Z","shell.execute_reply":"2025-01-07T03:14:36.703904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming df_test contains both the SMILES and target values\ndf_val_new, df_train_new = train_test_split(df_train, test_size=0.2, random_state=42, shuffle=True, stratify=df_train[\"binds_sEH\"])\n\n# Generate metadata descriptors aligned with the split data\nmetadata_train_new = np.array([calculate_descriptors(smiles) for smiles in df_train_new['molecule_smiles']])\nmetadata_val_new = np.array([calculate_descriptors(smiles) for smiles in df_val_new['molecule_smiles']])\nmetadata_test = np.array([calculate_descriptors(smiles) for smiles in df_test['molecule_smiles']])\n\n# Prepare your X and y datasets accordingly\nX_train_new = np.array([get_ecfp_vector(smiles) for smiles in df_train_new['molecule_smiles']])\nX_val_new = np.array([get_ecfp_vector(smiles) for smiles in df_val_new['molecule_smiles']])\nX_final_test = np.array([get_ecfp_vector(smiles) for smiles in df_test['molecule_smiles']])\n\ny_train_new = df_train_new['binds_BRD4'].values \ny_val_new = df_val_new['binds_BRD4'].values  # Replace 'target' with your actual column name\ny_final_test = df_test['is_BRD4'].values","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:14:36.705739Z","iopub.execute_input":"2025-01-07T03:14:36.705992Z","iopub.status.idle":"2025-01-07T03:15:20.857823Z","shell.execute_reply.started":"2025-01-07T03:14:36.705959Z","shell.execute_reply":"2025-01-07T03:15:20.857091Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, BatchNormalization, Lambda, Layer, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Precision, Recall, AUC, F1Score\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras import regularizers\n# Hyperparameters\ninput_dim = 1024  # Size of ECFP vector\nD_MODEL = 128    # Transformer model size (embedding dimension)\nNUM_HEADS = 2 # Number of attention heads\nDFF = 256        # Feed-forward network hidden layer size\nNUM_LAYERS = 4    # Number of Transformer layers\nDROPOUT_RATE = 0.3\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def call(self, q, k, v, mask):\n        # Split the heads and perform scaled dot-product attention\n        pass  # Implement following the tutorial\n        \n# 1. Transformer Encoder Layer (No need for embedding or positional encoding for ECFP)\nclass TransformerEncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=DROPOUT_RATE):\n        super(TransformerEncoderLayer, self).__init__()\n        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.ffn = tf.keras.Sequential([\n            Dense(dff, activation='relu'),\n            Dense(d_model)\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n    \n    def call(self, x, training=False):  # training flag added here\n        x_norm = self.layernorm1(x)\n        attn_output = self.mha(x_norm, x_norm)  # Attention layer\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = x+attn_output\n\n        out1_norm = self.layernorm2(out1)\n        ffn_output = self.ffn(out1_norm)  # Feed-forward network\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return out1_norm+ffn_output\n\n# 2. Full Transformer Encoder (No embedding layer, directly use ECFP input)\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, rate=DROPOUT_RATE):\n        super(TransformerEncoder, self).__init__()\n        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = Dropout(rate)\n\n    def call(self, x, training=False):\n        x = tf.cast(x, dtype=tf.float32) \n        x = self.dropout(x, training=training)\n\n      ######  # Reshape the input to add a sequence dimension (batch_size, 1, input_dim)x = tf.expand_dims(x, axis=1)\n\n        for enc_layer in self.enc_layers:\n            x = enc_layer(x, training=training)\n\n        return x\n    \nclass GlobalAveragePooling1D(Layer):\n    def __init__(self):\n        super(GlobalAveragePooling1D, self).__init__()\n\n    def call(self, inputs):\n        return tf.reduce_mean(inputs, axis=1)  \n\n\n# 3. Binding Affinity Prediction Model\nclass PositionalEncoding(Layer):\n    def __init__(self, input_dim, d_model):\n        super(PositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.pos_encoding = self.get_positional_encoding(input_dim, d_model)\n    \n    def get_positional_encoding(self, input_dim, d_model):\n        def get_angles(pos, i, d_model):\n            angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n            return pos * angle_rates\n\n        angle_rads = get_angles(np.arange(input_dim)[:, np.newaxis],\n                                np.arange(d_model)[np.newaxis, :],\n                                d_model)\n\n        # Apply sin to even indices (2i) and cos to odd indices (2i+1)\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n        return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n    def call(self, inputs):\n        # Add positional encoding to the input\n        seq_len = tf.shape(inputs)[1]\n        return inputs + self.pos_encoding[:, :seq_len, :]\n'''# Create a matrix of shape (input_dim, d_model)\n        pos = np.arange(input_dim)[:, np.newaxis]\n        i = np.arange(d_model)[np.newaxis, :]\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n        \n        # Apply the sine to even indices and cosine to odd indices\n        pos_encoding = np.zeros((input_dim, d_model))\n        pos_encoding[:, 0::2] = np.sin(pos * angle_rates[:, 0::2])\n        pos_encoding[:, 1::2] = np.cos(pos * angle_rates[:, 1::2])\n        \n        pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n        return pos_encoding\n    '''\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super().__init__()\n        self.d_model = tf.cast(d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    def get_config(self):\n        return {\n            'd_model': int(self.d_model),\n            'warmup_steps': int(self.warmup_steps)\n        }\n\ndef create_model():\n    inputs = Input(shape=(input_dim,), dtype=tf.float32)  # Input shape is the size of ECFP\n    x = Dense(D_MODEL, activation=\"relu\", kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.02))(inputs)  # Project input to d_model size (128)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x)\n    pos_encoding = PositionalEncoding(input_dim = 1, d_model = D_MODEL)\n    x = pos_encoding(x)\n    \n    encoder = TransformerEncoder(num_layers=NUM_LAYERS, d_model=D_MODEL, num_heads=NUM_HEADS, dff=DFF)\n    enc_output = encoder(x, training=True)  # Pass training argument explicitly\n    x = GlobalAveragePooling1D()(enc_output)  # Apply custom global average pooling\n\n    x = Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.02))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    ##################\n    \n    metadata_input = Input(shape=(metadata_train_new.shape[1],), name=\"Metadata_Input\")  # Ensure correct number of features\n    y = Dense(32, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.02))(metadata_input)\n    y = BatchNormalization()(y)\n    y = Dropout(0.5)(y)\n    y = Dense(32, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.02))(y)\n    \n    combined = concatenate([x, y])\n    \n    z = Dense(32, activation=\"relu\")(combined)\n    outputs = Dense(1, activation='sigmoid')(z)  # Adjust units based on your number of classes\n\n    model = Model(inputs=[inputs, metadata_input], outputs=outputs)\n    \n    learning_rate = CustomSchedule(d_model= D_MODEL)\n    #optimizer = Adam(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n    #    initial_learning_rate=0.0001, decay_steps=10000, decay_rate=0.4))\n    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC()])\n    \n    return model\n\n\n# Initialize the model\nmodel = create_model()\n\n##################\n#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n#X_train = scaler.fit_transform(X_train)\n\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n\n\n#############\n\n\nmodel.fit([X_train_new, metadata_train_new], y_train_new, \n          epochs=12, batch_size = 64,\n          validation_data=([X_val_new, metadata_val_new], y_val_new), callbacks=[early_stopping, model_checkpoint])\n\n#EVALUATE\n\nloss, accuracy, precision, recall, AUC= model.evaluate([X_final_test, metadata_test], y_final_test)\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")\nprint(f\"Test Precision: {precision}\")\nprint(f\"Test Recall: {recall}\")\nprint(f\"Test AUC: {AUC}\")\n\n\n\ny_final_test = np.round(y_final_test).astype(\"int32\")\n\ny_pred = (model.predict([X_final_test, metadata_test]) > 0.3).astype(\"int32\")\n\n\ncm = confusion_matrix(y_final_test, y_pred)\ndisplay = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisplay.plot(cmap=plt.cm.Blues)\nplt.title(\"Transformer Confusion Matrix\")\nplt.show()\n# pre-layer normalization, class-weights, F1score","metadata":{"execution":{"iopub.status.busy":"2025-01-07T03:15:20.859075Z","iopub.execute_input":"2025-01-07T03:15:20.859375Z","iopub.status.idle":"2025-01-07T03:16:22.106561Z","shell.execute_reply.started":"2025-01-07T03:15:20.859347Z","shell.execute_reply":"2025-01-07T03:16:22.105657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.nn import BCEWithLogitsLoss\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.data import Data, Batch\nfrom sklearn.metrics import f1_score, precision_recall_curve\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // num_heads\n\n        self.wq = nn.Linear(d_model, d_model)\n        self.wk = nn.Linear(d_model, d_model)\n        self.wv = nn.Linear(d_model, d_model)\n\n        self.dense = nn.Linear(d_model, d_model)\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n        x = x.view(batch_size, -1, self.num_heads, self.depth)\n        return x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, depth)\n\n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n\n        q = self.split_heads(self.wq(q), batch_size)\n        k = self.split_heads(self.wk(k), batch_size)\n        v = self.split_heads(self.wv(v), batch_size)\n\n        # Scaled dot-product attention\n        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n        dk = torch.tensor(k.size(-1), dtype=torch.float32)\n        scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n\n        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)\n        output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n\n        # Concatenate heads and pass through the dense layer\n        output = output.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len, num_heads, depth)\n        output = output.view(batch_size, -1, self.d_model)  # (batch_size, seq_len, d_model)\n        return self.dense(output)\n        \ndef custom_collate_fn(batch):\n    graphs, ecfp_data, metadata, labels = zip(*batch)  # Unpack the batch into separate components\n    batched_graphs = Batch.from_data_list(graphs)  # Batch the graphs using PyG's Batch\n    return (\n        batched_graphs, \n        torch.stack(ecfp_data), \n        torch.stack(metadata), \n        torch.stack(labels)\n    )\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Using device: {device}\")\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dff, rate=0.3):\n        super(TransformerEncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, dff),\n            nn.ReLU(),\n            nn.Linear(dff, d_model)\n        )\n        self.layernorm1 = nn.LayerNorm(d_model)\n        self.layernorm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(rate)\n        self.dropout2 = nn.Dropout(rate)\n\n    def forward(self, x, training=True):\n        attn_output = self.mha(x, x, x)  # Self-attention\n        attn_output = self.dropout1(attn_output)\n        out1 = self.layernorm1(x + attn_output)  # Add & Norm\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)  # Add & Norm\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.3):\n        super(TransformerEncoder, self).__init__()\n        self.enc_layers = nn.ModuleList([\n            TransformerEncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(rate)\n\n    def forward(self, x, training=True):\n        for layer in self.enc_layers:\n            x = layer(x)\n        return x\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, input_dim):\n        super(AttentionPooling, self).__init__()\n        self.attention_weights = nn.Linear(input_dim, 1)\n\n    def forward(self, x):\n        # x: (Batch, seq_len, d_model)\n        weights = F.softmax(self.attention_weights(x), dim=1)  # (Batch, seq_len, 1)\n        pooled_output = torch.sum(weights * x, dim=1)  # Weighted sum (Batch, d_model)\n        return pooled_output\n\nimport math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.encoding = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        self.encoding[:, 0::2] = torch.sin(position * div_term)\n        self.encoding[:, 1::2] = torch.cos(position * div_term)\n        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n\n    def forward(self, x):\n        # Add positional encoding to input\n        return x + self.encoding[:, :x.size(1), :].to(x.device)\n\nclass EarlyStopping:\n    def __init__(self, patience=10, monitor='loss', mode='min', delta=0.001, verbose = False, path=None):\n        \"\"\"\n        Initialize the EarlyStopping object.\n\n        Args:\n        - patience: Number of epochs to wait for improvement.\n        - monitor: Metric to monitor ('loss' or 'f1').\n        - mode: 'min' for loss, 'max' for F1 score.\n        - delta: Minimum change to qualify as an improvement.\n        \"\"\"\n        self.patience = patience\n        self.monitor = monitor\n        self.mode = mode\n        self.delta = delta\n        self.best_score = None\n        self.counter = 0\n        self.early_stop = False\n        self.verbose = verbose\n        self.path = path\n\n    def __call__(self, metric_value, model):\n        if self.best_score is None:\n            self.best_score = metric_value\n            self.save_checkpoint(model)\n            if self.verbose:\n                print(f\"Validation {self.monitor} improved, saving model.\")\n        elif self.is_improvement(metric_value):\n            self.best_score = metric_value\n            self.counter = 0\n            self.save_checkpoint(model)\n            if self.verbose:\n                print(f\"Validation {self.monitor} improved, saving model.\")\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n    def is_improvement(self, metric_value):\n        if self.mode == 'min':\n            return metric_value < self.best_score - self.delta\n        elif self.mode == 'max':\n            return metric_value > self.best_score + self.delta\n\n    def save_checkpoint(self, model):\n        if self.path:  \n            torch.save(model.state_dict(), self.path)\n            if self.verbose:\n                print(f\"Model saved to {self.path}\")\nclass CombinedGNNTransformer(nn.Module):\n    def __init__(self, gnn_model, ecfp_dim, metadata_dim, transformer_params, num_classes, hidden_dim, max_seq_len):\n        super(CombinedGNNTransformer, self).__init__()\n        # GNN Component\n        self.gnn = gnn_model\n\n        self.fc_metadata = nn.Sequential(nn.Linear(metadata_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)\n                                        )\n        self.ecfp_projection = nn.Linear(ecfp_dim, transformer_params['d_model'])\n        self.positional_encoding = PositionalEncoding(d_model = transformer_params['d_model'], max_len = max_seq_len)\n        # Transformer Component\n        self.transformer = TransformerEncoder(\n            num_layers=transformer_params['num_layers'],\n            d_model=transformer_params['d_model'],\n            num_heads=transformer_params['num_heads'],\n            dff=transformer_params['dff'],\n            rate=transformer_params['dropout_rate']\n        )\n        self.attention_pooling = AttentionPooling(transformer_params['d_model'])\n        self.fc_transformer = nn.Linear(transformer_params['d_model'], hidden_dim)\n\n        # Fully Connected Layer for Combined Output\n        self.fc_combined = nn.Linear(257, num_classes)##############gnn_output.size(1) + transformer_output.size(1) + metadata_output.size(1)\n\n    def forward(self, graph_data, ecfp_data, metadata):\n        # Graph Data\n        gnn_output = self.gnn(graph_data)  # GNN Output (Batch, 64)\n\n        ecfp_projected = self.ecfp_projection(ecfp_data).unsqueeze(1)  # (Batch, fingerprint_dim) -> (Batch, hidden_dim)\n        ecfp_with_pos = self.positional_encoding(ecfp_projected)\n        \n        # Metadata Data\n        metadata_output = F.relu(self.fc_metadata(metadata))\n\n        # ECFP Data\n        transformer_output = self.transformer(ecfp_with_pos)  # Transformer Output (Batch, Seq_len, D_MODEL)\n        transformer_output = self.attention_pooling(transformer_output)\n        transformer_output = F.relu(self.fc_transformer(transformer_output))  # FC on Transformer Output\n\n       \n        # Combine Outputs\n        combined = torch.cat((gnn_output, transformer_output, metadata_output), dim=1)  # Concatenate\n        combined_output = self.fc_combined(combined)  # Final Output\n        return combined_output\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass BindingAffinityDatasetWithGNN(Dataset):\n    def __init__(self, graph_data, ecfp_data, metadata, labels):\n        assert len(graph_data) == len(ecfp_data) == len(metadata) == len(labels), \\\n            f\"Dataset lengths mismatch: graphs={len(graph_data)}, ecfps={len(ecfp_data)}, metadata={len(metadata)}, labels={len(labels)}\"\n        \"\"\"\n        Args:\n            graph_data (list of Data): PyTorch Geometric Data objects representing molecular graphs.\n            fingerprints (torch.Tensor): Tensor of molecular fingerprints (e.g., ECFPs) of shape (N, fingerprint_dim).\n            metadata (torch.Tensor): Tensor of molecular metadata of shape (N, metadata_dim).\n            labels (torch.Tensor): Tensor of target labels of shape (N,).\n        \"\"\"\n        self.graph_data = graph_data\n        self.ecfp_data = ecfp_data\n        self.metadata = metadata\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return (\n            self.graph_data[idx],     # PyTorch Geometric Data object\n            torch.tensor(self.ecfp_data[idx], dtype=torch.float32),\n            torch.tensor(self.metadata[idx], dtype=torch.float32),\n            torch.tensor(self.labels[idx], dtype=torch.float32),       # Label\n        )\n\ndef tune_threshold(y_true, y_pred):\n    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n    best_threshold = thresholds[np.argmax(f1_scores)]\n    return best_threshold\n\n#\n\ndf_train, df_val = train_test_split(\n    df_train, test_size=0.2, random_state=42, stratify=df_train['binds_sEH'] if 'binds_sEH' in df_train.columns else y_train\n)\n\n\ndf_final_test = df_test.copy()\n\n\nmetadata_train = np.array([calculate_descriptors(smiles) for smiles in df_train['molecule_smiles']])\nmetadata_val = np.array([calculate_descriptors(smiles) for smiles in df_val['molecule_smiles']])\nmetadata_test = np.array([calculate_descriptors(smiles) for smiles in df_final_test['molecule_smiles']])\n\n\nX_train = np.array([get_ecfp_vector(smiles) for smiles in df_train['molecule_smiles']])\nX_val = np.array([get_ecfp_vector(smiles) for smiles in df_val['molecule_smiles']])\nX_final_test = np.array([get_ecfp_vector(smiles) for smiles in df_final_test['molecule_smiles']])\n\ny_train = df_train['binds_BRD4'].values\ny_val = df_val['binds_BRD4'].values\ny_final_test = df_final_test['is_BRD4'].values\n\ngraphs_train = []\nfor smiles in df_train['molecule_smiles']: \n    graph = smiles_to_graph(smiles)\n    if graph is not None:\n        graphs_train.append(graph)\ngraphs_val = []\nfor smiles in df_val['molecule_smiles']: \n    graph = smiles_to_graph(smiles)\n    if graph is not None:\n        graphs_val.append(graph)\ngraphs_test = []\nfor smiles in df_final_test['molecule_smiles']: \n    graph = smiles_to_graph(smiles)\n    if graph is not None:\n        graphs_test.append(graph)\n        \n'''from torch_geometric.data import Data\n\nfor i, graph in enumerate(graphs_train):\n    graph.y = torch.tensor([y_train[i]], dtype=torch.float)  # Attach each label to its corresponding graph in training set\n\nfor i, graph in enumerate(graphs_test):\n    graph.y = torch.tensor([y_test[i]], dtype=torch.float) '''\n\n\n\n #graphs_test, metadata_Test, X_final_test, y_final_test\n\n\n\n\n# Convert to tensors\nX_train, X_val, X_final_test = map(torch.tensor, (np.array(X_train), np.array(X_val), np.array(X_final_test)))\nmetadata_train, metadata_val, metadata_test = map(torch.tensor, (metadata_train, metadata_val, metadata_test))\ny_train, y_val, y_final_test = map(torch.tensor, (y_train, y_val, y_final_test))\n\n# Create PyTorch Datasets\ntrain_dataset = BindingAffinityDatasetWithGNN(graphs_train, X_train, metadata_train, y_train)\nval_dataset = BindingAffinityDatasetWithGNN(graphs_val, X_val, metadata_val, y_val)\ntest_dataset = BindingAffinityDatasetWithGNN(graphs_test, X_final_test, metadata_test, y_final_test)\n\n# Create PyTorch Geometric DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn = custom_collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn = custom_collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn = custom_collate_fn)\nprint(len(val_loader))\n\n# Initialize the model\nmodel = CombinedGNNTransformer(\n    gnn_model=GNN(),\n    ecfp_dim=X_train.shape[1],\n    metadata_dim=metadata_train.shape[1],\n    hidden_dim=128,\n    transformer_params={\n        'd_model': 128,\n        'num_heads': 2,\n        'dff': 256,\n        'num_layers': 4,\n        'dropout_rate': 0.3\n    },\n    num_classes = 1,\n    max_seq_len= 1024\n)\nmodel = model.to(device)\n\n# Loss and optimizer\ncriterion = BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n##########graph to see how train_loss and val_loss are changing\n#from torch.optim.lr_scheduler import ReduceLROnPlateau\n#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=10, verbose=True)\nfrom transformers import get_cosine_schedule_with_warmup\ntotal_steps = len(train_loader) * 12\nwarmup_steps = int(0.1 * total_steps)  # 10% warm-up\nscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\nearly_stopping = EarlyStopping(patience=3, verbose=True, path=\"best_model.pth\")\n# Training loop\nepochs = 12\nbest_threshold = 0.3\nsmoothing_factor = 0.2\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    all_train_preds, all_train_labels = [], []\n    for batch in train_loader:\n        graph_data = batch[0].to(device)\n        ecfp_data = batch[1].to(device)\n        metadata = batch[2].to(device)        \n        labels = batch[3].to(device)    \n        # Forward pass\n        outputs = model(graph_data, ecfp_data, metadata).squeeze()\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)#################\n\n        \n        optimizer.step()\n\n        train_loss += loss.item()\n        \n        all_train_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n        all_train_labels.extend(labels.cpu().numpy().flatten())\n    all_train_preds_binary = [1 if p > best_threshold else 0 for p in all_train_preds]\n    accuracy = accuracy_score(all_train_labels, all_train_preds_binary)\n    precision = precision_score(all_train_labels, all_train_preds_binary)\n    recall = recall_score(all_train_labels, all_train_preds_binary)\n    f1 = f1_score(all_train_labels, all_train_preds_binary)\n    print(f'Epoch {epoch+1}, Train Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n    # Validation step\n    model.eval()\n    val_loss = 0.0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            graph_data = batch[0].to(device)\n            ecfp_data = batch[1].to(device)\n            metadata = batch[2].to(device)        \n            labels = batch[3].to(device)    \n\n            outputs = model(graph_data, ecfp_data, metadata).squeeze()\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            \n            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy().flatten())\n    val_loss /= len(val_loader)\n    scheduler.step(val_loss)\n\n    new_threshold = tune_threshold(all_labels, all_preds)\n    best_threshold = (smoothing_factor * new_threshold) + (1-smoothing_factor) * best_threshold\n    val_preds_binary = [1 if p > best_threshold else 0 for p in all_preds]\n  \n    val_accuracy = accuracy_score(all_labels, val_preds_binary)\n    val_precision = precision_score(all_labels, val_preds_binary)\n    val_recall = recall_score(all_labels, val_preds_binary)\n    val_f1 = f1_score(all_labels, val_preds_binary)\n    print(f'Epoch {epoch+1}, Val Accuracy: {val_accuracy:.4f}, Threshold: {best_threshold:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val f1: {val_f1:.4f}')\n    print(\"\")\n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n\nmodel.eval()\ntest_loss = 0.0\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        graph_data = batch[0].to(device)\n        ecfp_data = batch[1].to(device)\n        metadata = batch[2].to(device)        \n        labels = batch[3].to(device)    \n\n        outputs = model(graph_data, ecfp_data, metadata).squeeze()\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(torch.sigmoid(outputs).cpu().numpy())\n\n# Compute metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n\ny_pred_binary = [1 if p > best_threshold else 0 for p in y_pred]\naccuracy = accuracy_score(y_true, y_pred_binary)\nprecision = precision_score(y_true, y_pred_binary)\nrecall = recall_score(y_true, y_pred_binary)\nroc_auc = roc_auc_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred_binary)\n\nprint(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, AUC: {roc_auc:.4f}, F1: {f1:.4f}\")\n\ncm = confusion_matrix(y_true, y_pred_binary)\ndisplay = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisplay.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T03:16:22.108329Z","iopub.execute_input":"2025-01-07T03:16:22.108795Z","iopub.status.idle":"2025-01-07T03:18:48.301529Z","shell.execute_reply.started":"2025-01-07T03:16:22.108754Z","shell.execute_reply":"2025-01-07T03:18:48.300432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correlation_matrix = df[['vector1', 'vector2', 'vector3']].corr()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T03:18:48.302756Z","iopub.execute_input":"2025-01-07T03:18:48.303116Z","iopub.status.idle":"2025-01-07T03:18:48.381252Z","shell.execute_reply.started":"2025-01-07T03:18:48.303076Z","shell.execute_reply":"2025-01-07T03:18:48.379559Z"}},"outputs":[],"execution_count":null}]}